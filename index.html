
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chuhan Li</title>
  
  <meta name="author" content="Chuhan Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="favicon.ico">
</head>

<body>
<!-- <body style="background-color:rgb(240,240,240);"> -->
	<table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr style="padding:0px">
	<td style="padding:0px">

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr style="padding:0px">

			<td style="padding:2.5%;width:60%;vertical-align:middle">
				<p style="text-align:left">
					<name>Chuhan Li ÈªéÊ•öÊ∂µ</name>
				</p>
        		Ph.D. Student 
            @<strong><a href="http://nlp.cs.ucsb.edu/">UCSB NLP</a></strong>
          

            <br>
        		Department of Computer Science, <strong><a href="https://www.ucsb.edu/">UC Santa Barbara</a></strong>
      			
      			<p></p>

                <strong>Email:</strong> chuhan_li [at] ucsb [dot] edu <br>
                <strong>Office:</strong> 2113 Henley Hall
      			
      			<p></p>

      			<div style="text-align:center">
        			<strong><a href="https://scholar.google.com/citations?user=Q1O7DtAAAAAJ&hl=en">Google Scholar</a></strong>
        			|
        			<strong><a href="https://x.com/_Chuhan_Li">X</a></strong>
        			|
              <strong><a href="https://www.linkedin.com/in/hugo-chuhan-li/">LinkedIn</a></strong>
              |
        			<strong><a href="https://github.com/LeeChuh">GitHub</a></strong>
      			</div>
			</td>

			<td style="width:20%;max-width:20%">
  				<img style="width:100%;max-width:90%" alt="profile photo" src="images/headshot.jpeg" class="hoverZoomLink">
			</td>
      <!-- <td style="width:20%;max-width:20%">
        <img style="width:100%;max-width:100%" alt="profile photo" src="images/p.jpg" class="hoverZoomLink">
    </td> -->

		</tr>
	</tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          I am a Ph.D. student in Computer Science at UC Santa Barbara, working with <strong><a href="https://eric-xw.github.io">Xin (Eric) Wang</a></strong>. Before that, I earned an <i>M.S. in Computer Science</i> at Yale University, working with <strong><a href="https://armancohan.com/">Arman Cohan</a></strong> and <strong><a href="https://www.cs.yale.edu/homes/ying-rex/">Rex Ying</a></strong>. Prior to that, I graduated <i>Summa Cum Laude</i> from Boston University with a <i>B.A. in Computer Science</i> and <i>B.A. in Mathematics</i>, where I worked with <strong><a href="https://cs-people.bu.edu/evimaria/">Evimaria Terzi</a></strong>.   
          <br><br>
          

          My research goal is to develop intelligent systems that <b>reason</b> and <b>interact</b> with the physical world. My primary focus lies in <b>machine learning</b>, <b>natural language processing</b>, and <b>computer vision</b>, particularly in <b>spatial intelligence</b>, <b>multimodal reasoning</b>, and <b>neuro-symbolic reasoning</b>. My recent research interests include: 
          <ul>
            <li><b>Evaluation</b> and <b>Analysis</b> of spatial-temporal capabilities in foundation models</li>
            <li><b>Post-Training</b> and <b>Agentic Systems</b> for enhancing spatial understanding and reasoning</li>
            <li><b>Robot Learning</b> for human interaction in the real world</li>
          </ul>


<!-- 
          My research interests include <strong>multimodal reasoning</strong> and <strong>neuro-symbolic reasoning</strong>. 
          
          In the short term, I aim to enhance foundation models' capabilities in spatial intelligence, spatial-temporal reasoning, and domain-specific expertise across multiple modalities (images, videos).

          In the long term, I want to make AI systems to be able to interact with physical world via multimodal data. -->

         </td>
       </tr>
    </tbody></table>

	<!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
             <td style="padding:20px;width:100%;vertical-align:middle">
               <div style="color:#014240">
                 <strong>"With spatial intelligence, AI will understand the real world."</strong>
                 &nbsp;&nbsp
                 -- <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>. 
                 <p></p>
               </div>
             </td>
           </tr>
    </tbody></table> -->


<br>
<br>


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <heading style="color: #00356b; font-weight: bold;">Selected Publications</heading>
  </tr>
  <br><br>
  * denotes equal contribution.
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<br>

  <!-- HybridMind -->
  <tr onmouseout="uflow_stop()" onmouseover="uflow_start()">
    <td style="padding:10px;width:20%;vertical-align:middle">
          <img style="width:100%;max-width:100%" alt="hybridmind" src="projects/hybridmind.png">
      <script type="text/javascript">
        function uflow_start() {
          document.getElementById('uflow_image').style.opacity = "1";
        }

        function uflow_stop() {
          document.getElementById('uflow_image').style.opacity = "0";
        }
        uflow_stop()
      </script>
    </td>
    <td style="padding:10px;width:80%;vertical-align:middle">
      <papertitle>HybridMind: Meta Selection of Natural Language and Symbolic Language for Enhanced LLM Reasoning</papertitle>
      <br>
      <a href="https://sophiahan6.github.io/">
        <author>Simeng Han*</author>
      </a>,
      <a href="https://helloworldlty.github.io/">
        <author>Tianyu Liu*</author>
      </a>,
      <u><b>Chuhan Li*</b></u>,
      <author>Xuyuan Xiong</author>,
      <a href="https://armancohan.com/">
        <author>Arman Cohan</author>
      </a>
      <br>
      <em>COLM 2025 The First Workshop on the Application of LLM Explainability to Reasoning and Planning</em>
      <br>
      <strong><a href="https://arxiv.org/abs/2409.19381v5">Paper</a></strong>
      <!-- |
      <strong><a href="https://github.com/yale-nlp/M3SciQA">Code</a></strong>
      |
      <strong><a href="https://huggingface.co/datasets/yale-nlp/M3SciQA">Dataset</a></strong> -->
    </td>
  </tr>

  <!-- TOMATO -->
  <tr onmouseout="uflow_stop()" onmouseover="uflow_start()">
    <td style="padding:10px;width:20%;vertical-align:middle">
          <img style="width:100%;max-width:100%" alt="tomato" src="projects/TOMATO.png">
      <script type="text/javascript">
        function uflow_start() {
          document.getElementById('uflow_image').style.opacity = "1";
        }

        function uflow_stop() {
          document.getElementById('uflow_image').style.opacity = "0";
        }
        uflow_stop()
      </script>
    </td>
    <td style="padding:10px;width:80%;vertical-align:middle">
      <papertitle>üçÖ TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models</papertitle>
      <br>
      <a href="https://ziyaosg.github.io/">
        <author>Ziyao Shangguan*</author>
      </a>,
      <u><b>Chuhan Li*</b></u>,
      <a href="https://www.linkedin.com/in/eason-d-a23a302a9/">
        <author>Yuxuan Ding</author>
      </a>,
      <author>Yanan Zheng</author>
      ,
      <a href="https://yilunzhao.github.io/">
        <author>Yilun Zhao</author>
      </a>,
      <a href="https://www.tescafitzgerald.com/">
        <author>Tesca Fitzgerald</author>
      </a>,
      <a href="https://armancohan.com/">
        <author>Arman Cohan</author>
      </a>
      <br>
      <em>International Conference on Learning Representations (ICLR)</em>, 2025 
      <!-- <span class="highlight"><strong>Oral</strong></span> -->
      <br>
      <strong><a href="https://arxiv.org/abs/2410.23266">Paper</a></strong>
      |
      <strong><a href="https://github.com/yale-nlp/TOMATO">Code</a></strong>
      |
      <strong><a href="https://huggingface.co/datasets/yale-nlp/TOMATO">Dataset</a></strong>
    </td>
  </tr>

  <!-- M3SciQA -->
  <tr onmouseout="uflow_stop()" onmouseover="uflow_start()">
    <td style="padding:10px;width:20%;vertical-align:middle">
          <img style="width:100%;max-width:100%" alt="M3SciQA" src="projects/M3SciQA.png">
      <script type="text/javascript">
        function uflow_start() {
          document.getElementById('uflow_image').style.opacity = "1";
        }

        function uflow_stop() {
          document.getElementById('uflow_image').style.opacity = "0";
        }
        uflow_stop()
      </script>
    </td>
    <td style="padding:10px;width:80%;vertical-align:middle">
      <papertitle>M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models</papertitle>
      <br>
      <u><b>Chuhan Li*</b></u>,
      <a href="https://ziyaosg.github.io/">
        <author>Ziyao Shangguan*</author>
      </a>,
      <a href="https://yilunzhao.github.io/">
        <author>Yilun Zhao</author>
      </a>,
      <a href="https://www.linkedin.com/in/deyuan-li/">
        <author>Deyuan Li</author>
      </a>,
      <a href="https://yixinl7.github.io/">
        <author>Yixin Liu</author>
      </a>,
      <a href="https://armancohan.com/">
        <author>Arman Cohan</author>
      </a>
      <br>
      <em>Findings of the Association for Computational Linguistics: EMNLP 2024</em>
      <!-- <span class="highlight"><strong>Oral</strong></span> -->
      <br>
      <strong><a href="https://arxiv.org/abs/2411.04075">Paper</a></strong>
      |
      <strong><a href="https://github.com/yale-nlp/M3SciQA">Code</a></strong>
      |
      <strong><a href="https://huggingface.co/datasets/yale-nlp/M3SciQA">Dataset</a></strong>
    </td>
  </tr>


</tbody></table>

<br>
<br>


<!-- Professional Services -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <heading style="color: #00356b; font-weight: bold;">Professional Services</heading>
  </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">


  <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <strong>Workshop Organizer:</strong>
          <ul>
            <li>4th workshop on 
              <a href="https://nenlp.github.io/spr2025/"><strong>New England NLP</strong></a> (NENLP), Apr 2025
            </li>
          </ul>
          <strong>Conference Reviewer:</strong> ICLR 2025, ACL 2025
          <br> 
          <br>
          <strong>Workshop Reviewer:</strong> ICLR 2025 workshop on LLM Reason and Plan, ICLR 2025 workshop on SCI-FM, NeurIPS 2024 workshop on FM4Science
          </td>
        </tr>
</tbody>
</table>


<br>
<br>


<!-- Teaching  -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <heading style="color: #00356b; font-weight: bold;">Teaching</heading>
  </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">


          <tr>
            <td style="padding:10px;width:20%;vertical-align:middle">
                  <img style="width:100%;max-width:100%" alt="yale_logo" src="images/yale_logo.png">
            </td>
              <td style="padding:10px;width:80%;vertical-align:middle">
          	  Teaching Assistant, <strong><a href="https://nlp.cs.yale.edu/cpsc477/">Natural Language Processing</a></strong> (CPSC 477/577), Spring 2025
              <br>
              <br>
              Teaching Assistant, <strong><a href="https://graph-and-geometric-learning.github.io/cpsc483-583-website-24fall/#/">Deep Learning on Graph-Structured Data</a></strong> (CPSC 483/583), Fall 2024
              <br>
              <br>
              Teaching Assistant, <strong>Introduction to Machine Learning</strong> (CPSC 381), Spring 2024
              <br>
              <br>
              Teaching Assistant, <strong>Algorithms</strong> (CPSC 365), Fall 2023
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:20%;vertical-align:middle">
                  <img style="width:100%;max-width:100%" alt="bu_logo" src="images/bu_logo.png">
            </td>
              <td style="padding:10px;width:80%;vertical-align:middle">
              Teaching Assistant, <strong>Combinatoric Structures</strong> (CS 131), Fall 2021, Spring 2022, Spring 2023
              <br>
              <br>
              Teaching Assistant, <strong>Foundation of Data Science</strong> (CS 365), Fall 2022
              <br>
              <br>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right">
                Website design from <a href="https://jonbarron.info/">Jon Barron</a>, source code <a href="https://github.com/jonbarron/website/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
